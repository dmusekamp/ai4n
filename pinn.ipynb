{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18ef13f",
   "metadata": {},
   "source": [
    "# Expert  Tutorial: Introduction to Physics-Informed Neural Networks\n",
    "\n",
    "In this tutorial, we will learn the basics of Physics-Informed Neural Networks (PINNs). It consistes of three parts:\n",
    "1. Forward Problem - Heat Equation 2D\n",
    "2. Inverse Problem - Heat Equation 2D\n",
    "3. Application - Inverse Problem - Transfer Learning\n",
    "\n",
    "This tutorail is in the form of *jupyter* notebook hosted on Google Collab. Go through the tutorial step-by-step and try understand the code. In the places with a *TODO* it is your task to fill out the blanks!\n",
    "\n",
    "Let's get started! Don't hesitate to ask us any questions!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c591c",
   "metadata": {},
   "source": [
    "## Part 1: Forward Problem - Heat Equation 2D\n",
    "\n",
    "In this part, we will try to solve the following 1D Heat PDE with a PINN:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    u_t(x,t) &= \\kappa u_{xx}(x,t) \\\\\n",
    "    u(x, 0) &= \\sin(x) \\\\\n",
    "    u(0,t ) &= 0 \\\\\n",
    "    u(\\pi,t) &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This PDE is directly solvable, the exact solution is $u(x, t) = \\sin(x) e^{-\\kappa t}$. We will use it as a test case for the PINN. \n",
    "\n",
    "The subsections are as follows:\n",
    "1) Develop ML model\n",
    "2) Develop loss function\n",
    "3) Define computational grid\n",
    "3) Complete training loop and visualize \n",
    "\n",
    "- [Optional] - What happens if we change activation functions? (ReLU, GeLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e76d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# First import the necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3017938",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.1 Develop ML Model\n",
    "\n",
    "Model description:  \n",
    "$$\n",
    "u_\\theta = \\text{MLP}(x)\n",
    "$$\n",
    "\n",
    "The model is a three-layer fully connected neural network with 64 neurons per hidden layer and **tanh** activation functions:\n",
    "$$\n",
    "u_\\theta(x) = W_3 \\, \\tanh\\!\\left(W_2 \\, \\tanh\\!\\left(W_1 x + b_1\\right) + b_2\\right) + b_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6532631",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩  Develop the ML model in Torch\n",
    "class PINN(nn.Module):\n",
    "    # MLP: 3 layers, 64 neurons; Tahn activation function\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=1):\n",
    "        super().__init__() \n",
    "        # TODO: implement the model architecture\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9236696",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835e0a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        # TODO: implement the model architecture\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.Tanh()]\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc35542",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.2 Develop Loss Function - Residual Loss for the Heat Equation\n",
    "\n",
    "The total loss combines all three components:\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} =\n",
    "L_{\\text{res}} +\n",
    "L_{\\text{bc}} +\n",
    "L_{\\text{ic}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- **Residual loss** enforces the PDE:\n",
    "\n",
    "$$\n",
    "L_{\\text{res}} = \\frac{1}{N_r} \\sum_{i=1}^{N_r} \\left( u_t(x_i, t_i) - \\kappa \\, u_{xx}(x_i, t_i) \\right)^2\n",
    "$$\n",
    "\n",
    "- **Boundary condition loss** enforces the boundary:\n",
    "\n",
    "$$\n",
    "L_{\\text{bc}} = \\frac{1}{N_b} \\sum_{i=1}^{N_b} \\left( u_\\theta(x_b^i, t_b^i) - u_{\\text{BC}}^i \\right)^2\n",
    "$$\n",
    "\n",
    "- **Initial condition loss** enforces the initial state:\n",
    "\n",
    "$$\n",
    "L_{\\text{ic}} = \\frac{1}{N_i} \\sum_{i=1}^{N_i} \\left( u_\\theta(x_i, 0) - u_{\\text{IC}}^i \\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b47a2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩 Implement loss function\n",
    "\n",
    "def heat_residual_loss(model, collocation_pts, kappa=1):\n",
    "    # TODO: implement the residual loss for the heat equation\n",
    "    return residual_loss\n",
    "\n",
    "def heat_pinn_loss(model, collocation_pts, boundary_pts, initial_pts, kappa=1):\n",
    "\n",
    "    residual_loss = heat_residual_loss(model, collocation_pts, kappa)\n",
    "\n",
    "    #TODO: Implement the boundary loss. Use the squared error between the true BC and the model prediction on the BC points.\n",
    "\n",
    "    #TODO: Implement the initial condition loss. Use the squared error between the true IC and the model prediction on the IC points.\n",
    "    \n",
    "    total_loss = residual_loss + boundary_loss + initial_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110b679",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f16d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def heat_residual_loss(model, collocation_pts, kappa=1):\n",
    "    # TODO: implement the residual loss for the heat equation.\n",
    "    collocation_pts.requires_grad_(True)\n",
    "    u = model(collocation_pts)\n",
    "    grads = torch.autograd.grad(u, collocation_pts, torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = grads[:, 0:1]\n",
    "    u_t = grads[:, 1:2]\n",
    "    u_xx = kappa * torch.autograd.grad(u_x, collocation_pts, torch.ones_like(u_x), create_graph=True)[0][:, 0:1]\n",
    "    residual_loss =  ((u_t - u_xx) ** 2).mean()\n",
    "    return residual_loss\n",
    "\n",
    "def heat_pinn_loss(model, collocation_pts, boundary_pts, initial_pts, kappa=1):\n",
    "\n",
    "    residual_loss = heat_residual_loss(model, collocation_pts, kappa)\n",
    "\n",
    "    #TODO: Implement the boundary loss. Use the squared error between the true BC and the model prediction on the BC points.\n",
    "    bc_targets = torch.zeros(len(boundary_pts))\n",
    "    boundary_loss = ((model(boundary_pts) - bc_targets) ** 2).mean()\n",
    "\n",
    "\n",
    "    #TODO: Implement the initial condition loss. Use the squared error between the true IC and the model prediction on the IC points.\n",
    "    ic_targets = torch.sin(initial_pts[:, 0]).unsqueeze(1)  # Make it [100, 1]\n",
    "    initial_loss = ((model(initial_pts) - ic_targets) ** 2).mean()\n",
    "\n",
    "    total_loss = residual_loss + boundary_loss + initial_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0ed7f",
   "metadata": {},
   "source": [
    "### 1.3 Define Computational  Grid\n",
    "We need to sample points from the interior to calculate the residuals (called collocation points) and points on the IC and BC to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad73cf3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Domain\n",
    "x_min, x_max = 0.0, np.pi\n",
    "t_min, t_max = 0.0, 1.0\n",
    "\n",
    "# Collocation points (interior)\n",
    "n_colloc = 1000\n",
    "x_c = torch.rand(n_colloc, 1) * (x_max - x_min) + x_min\n",
    "t_c = torch.rand(n_colloc, 1) * (t_max - t_min) + t_min\n",
    "colloc = torch.cat([x_c, t_c], dim=1)\n",
    "\n",
    "# Initial condition: t=0, u(x,0)=sin(x)\n",
    "n_ic = 100\n",
    "ic_x = torch.linspace(x_min, x_max, n_ic).reshape(-1, 1)\n",
    "ic_t = torch.zeros_like(ic_x)\n",
    "ic_pts = torch.cat([ic_x, ic_t], dim=1)\n",
    "\n",
    "# Boundary condition: t=1, u(x,1)=0\n",
    "n_bc = 100\n",
    "t_bc = torch.linspace(t_min, t_max, n_bc).unsqueeze(1)\n",
    "bc_pts = torch.cat([\n",
    "    torch.cat([torch.zeros_like(t_bc), t_bc], dim=1),\n",
    "    torch.cat([torch.full_like(t_bc, np.pi), t_bc], dim=1)\n",
    "], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1712ec8",
   "metadata": {},
   "source": [
    "#### [Optional] - Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b007b2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Collocation points\n",
    "plt.scatter(colloc[:, 0].numpy(), colloc[:, 1].numpy(), color='blue', s=10, label='Collocation Points', alpha=0.5)\n",
    "\n",
    "# Initial condition points\n",
    "plt.scatter(ic_x.numpy(), ic_t.numpy(), color='green', s=30, label='Initial Condition (t=0)')\n",
    "\n",
    "# Boundary condition points\n",
    "plt.scatter(bc_pts[:, 0].numpy(), bc_pts[:, 1].numpy(), color='red', s=30, label='Boundary Conditions (x=0, pi)')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('PINN Training Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74122d",
   "metadata": {},
   "source": [
    "### 1.4 Training Loop\n",
    "Now let's combine everything and train the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5aafb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩 Implement training loop\n",
    "\n",
    "def train(model, colloc, ic_pts, bc_pts, loss_function, epochs=2000, lr=1e-3):\n",
    "\n",
    "    #TODO 0. Define Optimizer\n",
    "\n",
    "    history = [] # for logging the loss\n",
    "    for i in range(epochs):\n",
    "        #TODO 1.  zero the gradients\n",
    "\n",
    "        #TODO 2. compute the loss\n",
    "\n",
    "        ##TODO 3. backpropagate the loss\n",
    "\n",
    "        ##TODO 4. update the weights\n",
    "        \n",
    "        history.append(loss.item())\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Epoch {i}: Loss={loss.item():.4e}\")\n",
    "    return history\n",
    "\n",
    "# initialize model\n",
    "model = PINN(2, 1)\n",
    "# train model\n",
    "history = train(model, colloc, ic_pts, bc_pts, heat_pinn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066ae5e",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769a81f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, colloc, ic_pts, bc_pts, loss_function, epochs=2000, lr=1e-3):\n",
    "\n",
    "    # 0. Define Optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    history = []\n",
    "    for i in range(epochs):\n",
    "        # 1.  zero the gradients\n",
    "        opt.zero_grad()\n",
    "        # 2. compute the loss\n",
    "        loss = loss_function(model, colloc, bc_pts, ic_pts)\n",
    "        # 3. backpropagate the loss\n",
    "        loss.backward()\n",
    "        # 4. update the weights\n",
    "        opt.step()\n",
    "        history.append(loss.item())\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Epoch {i}: Loss={loss.item():.4e}\")\n",
    "    return history\n",
    "\n",
    "model = PINN(2, 1)\n",
    "history = train(model, colloc, ic_pts, bc_pts, heat_pinn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7f749",
   "metadata": {},
   "source": [
    "### 1.5 Test and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f350dd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def exact(x, t):\n",
    "    return np.sin(x) * np.exp(-t)\n",
    "\n",
    "def visualize(X, T, u_true, u_pred):\n",
    "    pts = torch.tensor(np.column_stack([X.flatten(), T.flatten()]), dtype=torch.float32)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.contourf(X, T, u_pred, 50, cmap='viridis')\n",
    "    plt.title('PINN Prediction')\n",
    "    plt.xlabel('x'); plt.ylabel('t')\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.contourf(X, T, u_true, 50, cmap='viridis')\n",
    "    plt.title('Exact Solution')\n",
    "    plt.xlabel('x'); plt.ylabel('t')\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.contourf(X, T, np.abs(u_pred - u_true), 50, cmap='Reds')\n",
    "    plt.title('Absolute Error')\n",
    "    plt.xlabel('x'); plt.ylabel('t')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Testgrid\n",
    "x_test = np.linspace(x_min, x_max, 101)\n",
    "t_test = np.linspace(t_min, t_max, 101)\n",
    "X, T = np.meshgrid(x_test, t_test)\n",
    "pts = torch.tensor(np.column_stack([X.flatten(), T.flatten()]), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    u_pred = model(pts).numpy().reshape(X.shape)\n",
    "u_true = exact(X, T)\n",
    "\n",
    "visualize(X, T, u_true, u_pred)\n",
    "\n",
    "print(f\"Max abs error: {np.abs(u_pred - u_true).max():.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ff2df",
   "metadata": {},
   "source": [
    "___\n",
    "## Part 2: Inverse Problem - Heat Equation 2D\n",
    "\n",
    "In inverse problems, we are given some data from the true process, and try to infer the parameters of the process that generated the data. However, we do now the underlying PDE. We can approach this problem using PINNs by adding the unknown physical PDE parameter as a variable to be optimized. Afterwards, we minimize the residual PDE loss and the loss to the data samples at the same time. \n",
    "\n",
    "The steps we will take are:\n",
    "- 2.1 Generate Synthetic data: Typically you want to have this data from real world observations; here we use synthetic data as an example.\n",
    "- 2.2 Define loss function\n",
    "- 2.3 Implement training loop\n",
    "- 2.4 Visualize\n",
    "\n",
    "- [Optional] - What happens if our real-world data is noisy? - Add noise to synthetic data and re-run\n",
    "- [Optional] -  What happens if we reduce the synthetic data quantity? -  decrease the  $51$ value in the $x_c$ and $t_c$ generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1031ec",
   "metadata": {},
   "source": [
    "### 2.1 Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d747bd6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 2.1 Synthetic data generation\n",
    "\n",
    "kappa_true = 2.4 # what we actually want to predict\n",
    "\n",
    "def exact(x, t):\n",
    "    return np.sin(x) * np.exp(-kappa_true * t)\n",
    "\n",
    "x_c = torch.linspace(x_min, x_max, 51)\n",
    "t_c = torch.linspace(t_min, t_max, 51)\n",
    "X, T = torch.meshgrid(x_c, t_c)\n",
    "colloc = torch.stack([X.flatten(), T.flatten()], dim=1)\n",
    "\n",
    "data = torch.Tensor(exact(X.numpy(), T.numpy())).flatten().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb9777",
   "metadata": {},
   "source": [
    "### 2.2 Modify Loss Function\n",
    "\n",
    "The total loss combines all three components:\n",
    "\n",
    "$$\n",
    "L_{\\mathrm{total}} =\n",
    "L_{\\mathrm{res}} +\n",
    "L_{\\mathrm{data}}\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "$$\n",
    "L_{\\mathrm{data}} \\text{ represents the available observational data.}\n",
    "$$\n",
    "\n",
    "Here, the missing information in the model (such as initial and boundary conditions)  \n",
    "is compensated for by incorporating additional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e47e80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩 2.2 Implement loss function\n",
    "def loss_function_inverse(model, kappa, collocation_pts, boundary_pts, initial_pts, data):\n",
    "\n",
    "    # Use the standard PINN loss\n",
    "    pde_loss = heat_pinn_loss(model, collocation_pts, boundary_pts, initial_pts, kappa)\n",
    "\n",
    "    #TODO: Add data loss term\n",
    "\n",
    "    return pde_loss + data_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0f9a9",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1505d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function_inverse(model, kappa, collocation_pts, boundary_pts, initial_pts, data):\n",
    "\n",
    "    # Use the standard PINN loss\n",
    "    pde_loss = heat_pinn_loss(model, collocation_pts, boundary_pts, initial_pts, kappa)\n",
    "\n",
    "    #TODO: Add data loss term\n",
    "    data_loss = (model(collocation_pts) - data).square().mean()\n",
    "    return pde_loss + data_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c817e6",
   "metadata": {},
   "source": [
    "### 2.3 Implement Training Loop\n",
    "Here in addition to adding the model's parameter to the optimizer, we need to include the material propertie $\\kappa$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120915c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩 Implement Training Loop\n",
    "\n",
    "model = PINN(2, 1)\n",
    "kappa = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) # (!) - important\n",
    "\n",
    "#TODO 0. implement the optimizer\n",
    "\n",
    "#TODO1.  create optimizer\n",
    "\n",
    "epochs = 5000\n",
    "history = []\n",
    "for i in range(epochs):\n",
    "    #TODO 2. zero the gradients\n",
    "\n",
    "    #TODO 3. compute the loss\n",
    "\n",
    "    #TODO 4. backpropagate the loss\n",
    "\n",
    "    #TODO 5. update the weights\n",
    "\n",
    "    history.append(loss.item())\n",
    "    if i % 200 == 0:\n",
    "        print(f\"Epoch {i}: Loss={loss.item():.4e}, kappa={kappa.item():.3f}\")\n",
    "\n",
    "pts = torch.tensor(np.column_stack([X.flatten(), T.flatten()]), dtype=torch.float32)\n",
    "\n",
    "visualize(X.numpy(), T.numpy(), exact(X.numpy(), T.numpy()), model(pts).detach().numpy().reshape(X.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40325978",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d15019",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = PINN(2, 1)\n",
    "kappa = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "#TODO: implement the training loop\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [kappa], lr=1e-3)\n",
    "epochs = 5000\n",
    "history = []\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function_inverse(model, kappa, colloc, bc_pts, ic_pts, data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history.append(loss.item())\n",
    "    if i % 200 == 0:\n",
    "        print(f\"Epoch {i}: Loss={loss.item():.4e}, kappa={kappa.item():.3f}\")\n",
    "\n",
    "pts = torch.tensor(np.column_stack([X.flatten(), T.flatten()]), dtype=torch.float32)\n",
    "\n",
    "visualize(X.numpy(), T.numpy(), exact(X.numpy(), T.numpy()), model(pts).detach().numpy().reshape(X.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa9723",
   "metadata": {},
   "source": [
    "## Part 3: Application - Inverse Problem - Transfer Learning\n",
    "Imagine you have a **mechanical system** — like a *vibrating beam*, a *spring–mass–damper*, or even a *sensor-mounted machine part* — that follows this physical law:\n",
    "\n",
    "\\[\n",
    "m x''(t) + c x'(t) + k x(t) = 0\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- **m**: mass *(known)*\n",
    "- **c**: damping coefficient *(unknown)*\n",
    "- **k**: stiffness *(unknown)*\n",
    "- **x(t)**: displacement *(measured or observed)*\n",
    "- **x'(t)**, **x''(t)**: velocity and acceleration\n",
    "\n",
    "In real experiments, you can’t directly measure **c** and **k**, but you can measure **displacement over time** with sensors.\n",
    "\n",
    "So, you have:\n",
    "\n",
    "- A known **model equation** (the physics)  \n",
    "- A few **noisy measurements** of \\( x(t) \\)  \n",
    "- Two **unknown physical parameters** (\\( c \\), \\( k \\)) you want to identify\n",
    "\n",
    "The following tutorial is structured as:\n",
    "1) Generate ground truth solution\n",
    "2) From this ground truth solution we sample sparse measurements\n",
    "3) 🧩 [TODO] Construt a PINN Model\n",
    "4) 🧩 [TODO] Define the PINN Loss\n",
    "5) 🧩 [TODO] Train the model to solve the inverse problem (find  $c$ and $k$)\n",
    "6) Explore transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74f8f3",
   "metadata": {},
   "source": [
    "### 3.1 Generate Ground Truth Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd1b23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Analytical solution of the damped oscillator ---\n",
    "def damped_oscillator_solution_np(t, m, c, k, x0, v0):\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    omega_n = np.sqrt(k / m)\n",
    "    disc = c**2 - 4*m*k\n",
    "\n",
    "    if disc < 0:  # underdamped\n",
    "        wn = omega_n\n",
    "        zeta = c / (2*np.sqrt(m*k))\n",
    "        wd = wn * np.sqrt(max(0.0, 1 - zeta**2))\n",
    "        exp_term = np.exp(-zeta*wn*t)\n",
    "        C1, C2 = x0, (v0 + zeta*wn*x0)/wd\n",
    "        x = exp_term * (C1*np.cos(wd*t) + C2*np.sin(wd*t))\n",
    "        return x\n",
    "\n",
    "    elif np.isclose(disc, 0.0):  # critically damped\n",
    "        wn = omega_n\n",
    "        exp_term = np.exp(-wn*t)\n",
    "        x = (x0 + (v0 + wn*x0)*t) * exp_term\n",
    "        return x\n",
    "\n",
    "    else:  # overdamped\n",
    "        sqrt_disc = np.sqrt(disc)\n",
    "        r1 = (-c + sqrt_disc) / (2*m)\n",
    "        r2 = (-c - sqrt_disc) / (2*m)\n",
    "        denom = (r1 - r2)\n",
    "        A = (v0 - r2 * x0) / denom\n",
    "        B = x0 - A\n",
    "        x = A * np.exp(r1 * t) + B * np.exp(r2 * t)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 2. Generate the signal ---\n",
    "def generate_signal(m=1.0, c=0.5, k=4.0, x0=1.0, v0=0.0, T=10.0, n_points=400):\n",
    "    t = np.linspace(0, T, n_points)\n",
    "    x = damped_oscillator_solution_np(t, m, c, k, x0, v0)\n",
    "    return t, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b8ea3",
   "metadata": {},
   "source": [
    "#### [Optional] - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e9406",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Visualize\n",
    "t_signal, x_signal = generate_signal(m=1.0, c=0.5, k=4.0, x0=1.0, v0=0.0)\n",
    "plt.plot(t_signal, x_signal)\n",
    "plt.xlabel('Displacement')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Damped Oscillator Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3ad73",
   "metadata": {},
   "source": [
    "### 3.2 Collect Sparse Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd4524",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Sample synthetic sensor data ---\n",
    "def get_sensor_data(t_signal, x_signal, n_obs=21, noise_scale=0.005, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    t_obs = np.linspace(t_signal.min(), t_signal.max(), n_obs)\n",
    "    x_true = np.interp(t_obs, t_signal, x_signal)\n",
    "    x_obs = x_true + np.random.normal(scale=noise_scale, size=x_true.shape)\n",
    "    return t_obs, x_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f04cc2",
   "metadata": {},
   "source": [
    "#### [Optional] - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd4cd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. Visualize the true signal and data ---\n",
    "t_signal, x_signal = generate_signal(m=1.0, c=0.5, k=4.0, x0=1.0, v0=0.0)\n",
    "t_obs, x_obs = get_sensor_data(t_signal, x_signal, n_obs=10, noise_scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(t_signal, x_signal, label='True Signal', linewidth=2)\n",
    "plt.scatter(t_obs, x_obs, color='r', edgecolor='k', s=60, zorder=5, label='Sensor Data')\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Displacement $x(t)$')\n",
    "plt.title('Damped Oscillator Signal and Sensor Measurements')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad49f9d",
   "metadata": {},
   "source": [
    "### 3.3 Construct Model\n",
    "\n",
    "1. Create a torch class that models:\n",
    "$$x_\\theta = \\text{MLP}(t)$$\n",
    "\n",
    "2. Create a computer_derivative function that calculates x' and x''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113e0c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩 - Build a PINN model\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, hidden=64, layers=3):\n",
    "        super().__init__()\n",
    "        # [TODO] Define the neural network layers\n",
    "        # Hint: Use nn.Linear and nn.Tanh activations\n",
    "        # Example: [input → hidden → ... → output]\n",
    "        seq = __?__       # Fill in the sequence of layers\n",
    "        self.net = __?__  # Fill in: wrap the sequence with nn.Sequential\n",
    "\n",
    "    def forward(self, t):\n",
    "        # [TODO] Define the forward pass through the network\n",
    "        # Input: t (time), Output: x(t)\n",
    "        return __?__      # Fill in: use the defined network\n",
    "\n",
    "    def compute_derivatives(self, t):\n",
    "        \"\"\"\n",
    "        Compute first and second derivatives of x(t)\n",
    "        with respect to time t using autograd.\n",
    "        \"\"\"\n",
    "        # Ensure gradients are enabled for t\n",
    "        t = t.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # [TODO] Forward pass\n",
    "        x = __?__         # Fill in: model prediction x(t)\n",
    "\n",
    "        # [TODO] First derivative: dx/dt\n",
    "        x_t = torch.autograd.grad(\n",
    "            x, t,\n",
    "            grad_outputs=torch.ones_like(x),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        # [TODO] Second derivative: d²x/dt²\n",
    "        x_tt = __?__      # Fill in: autograd call for second derivative\n",
    "\n",
    "        return x, x_t, x_tt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cd074",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8451e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Solution\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, hidden=64, layers=3):\n",
    "        super().__init__()\n",
    "        seq = [nn.Linear(1, hidden), nn.Tanh()]\n",
    "        for _ in range(layers-1):\n",
    "            seq += [nn.Linear(hidden, hidden), nn.Tanh()]\n",
    "        seq += [nn.Linear(hidden, 1)]\n",
    "        self.net = nn.Sequential(*seq)\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "    def compute_derivatives(self, t):\n",
    "        \"\"\"\n",
    "        Compute first and second derivatives of x(t)\n",
    "        with respect to time t using autograd.\n",
    "        \"\"\"\n",
    "        # Ensure gradients are enabled for t\n",
    "        t = t.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Forward pass\n",
    "        x = self.forward(t)\n",
    "\n",
    "        # First derivative: dx/dt\n",
    "        x_t = torch.autograd.grad(\n",
    "            x, t,\n",
    "            grad_outputs=torch.ones_like(x),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        # Second derivative: d²x/dt²\n",
    "        x_tt = torch.autograd.grad(\n",
    "            x_t, t,\n",
    "            grad_outputs=torch.ones_like(x_t),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        return x, x_t, x_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c547af",
   "metadata": {},
   "source": [
    "### 3.4. Define the PINN Loss\n",
    "\n",
    "In this section, students will implement the **loss function** for a Physics-Informed Neural Network (PINN) modeling a damped oscillator:\n",
    "\n",
    "\\[\n",
    "m x''(t) + c x'(t) + k x(t) = 0\n",
    "\\]\n",
    "\n",
    "### Guidance:\n",
    "\n",
    "- **Trainable parameters:**  \n",
    "  `damping_param` (c) and `stiffness_param` (k) are the unknown physical coefficients to learn.\n",
    "\n",
    "- **Inputs:**  \n",
    "  - `t_coll` → collocation points used to enforce the PDE residual.  \n",
    "  - `t_sensors` & `x_sensors` → sensor measurement times and corresponding observed displacements.  \n",
    "  - `ic_time`, `ic_displacement`, `ic_velocity` → initial conditions at time `t=0`.\n",
    "\n",
    "- **Loss components:**  \n",
    "  1. **Residual loss:** Enforce the differential equation at collocation points.  \n",
    "  2. **Data loss:** Match model predictions to sensor measurements.  \n",
    "  3. **Initial condition loss:** Ensure the model satisfies the initial displacement and velocity.\n",
    "\n",
    "- **Derivatives:**  \n",
    "  Use `torch.autograd.grad` to compute the first and second derivatives of `x(t)` w.r.t. time.\n",
    "\n",
    "- **Combine losses:**  \n",
    "  Use weighted sum to balance residual, data, and IC contributions.\n",
    "\n",
    "> 💡 Hint: Start by computing the model output at each set of points, then its derivatives, and finally the squared errors for each loss term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08891efb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 🧩 Exercise: Implement the PINN Loss Function \n",
    "def loss_fn(model, c, k,\n",
    "            t_coll, t_obs, y_obs,\n",
    "            t_ic, x0_t, v0_t,\n",
    "            m):\n",
    "    \"\"\"\n",
    "    Compute the total loss for a damped oscillator PINN.\n",
    "\n",
    "    The loss combines:\n",
    "        (1) Physics residual loss:   m x'' + c x' + k x = 0\n",
    "        (2) Data loss:               match observed data\n",
    "        (3) Initial condition loss:  enforce x(0) = x0, x'(0) = v0\n",
    "    \"\"\"\n",
    "\n",
    "    # ---[TODO] 1. Physics residual loss ---\n",
    "    x_pred, x_t, x_tt = __?__   # Fill in: use model.compute_derivatives(...)\n",
    "    res = __?__                 # Fill in: m*x_tt + c*x_t + k*x_pred\n",
    "    l_res = __?__               # Fill in: mean squared residual\n",
    "\n",
    "    # ---[TODO] 2. Data fitting loss ---\n",
    "    y_pred = __?__              # Fill in: model prediction at t_obs\n",
    "    l_data = __?__              # Fill in: mean squared data loss\n",
    "\n",
    "    # ---[TODO] 3. Initial condition loss ---\n",
    "    x_ic, x_t_ic, _ = __?__     # Fill in: model.compute_derivatives(t_ic)\n",
    "    l_ic = __?__                # Fill in: IC loss for displacement and velocity\n",
    "\n",
    "    # ---[TODO] 4. Total loss ---\n",
    "    loss = __?__                # Fill in: combine all losses\n",
    "\n",
    "    return loss, l_res, l_data, l_ic, c, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474caba",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c1fb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "def loss_fn(model, c, k, t_coll, t_obs, y_obs, t_ic, x0_t, v0_t, m):\n",
    "    # --- 1. Physics residual loss ---\n",
    "    x_pred, x_t, x_tt = model.compute_derivatives(t_coll)\n",
    "    # Damped oscillator residual: m*x_tt + c*x_t + k*x = 0\n",
    "    res = m * x_tt + c * x_t + k * x_pred\n",
    "    l_res = torch.mean(res**2)\n",
    "\n",
    "    # --- 2. Data fitting loss ---\n",
    "    y_pred = model(t_obs)\n",
    "    l_data = torch.mean((y_pred - y_obs) ** 2)\n",
    "\n",
    "    # --- 3. Initial condition loss ---\n",
    "    x_ic, x_t_ic, _ = model.compute_derivatives(t_ic)\n",
    "    l_ic = torch.mean((x_ic - x0_t) ** 2) + torch.mean((x_t_ic - v0_t) ** 2)\n",
    "\n",
    "    # --- 4. Total loss ---\n",
    "    loss = l_res + l_data + l_ic\n",
    "\n",
    "    return loss, l_res, l_data, l_ic, c, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995a690",
   "metadata": {},
   "source": [
    "### 3.5 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd96e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 6. Synthetic data for PINN training ---\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# System parameters\n",
    "m, c_true, k_true = 1.0, 0.5, 2.0\n",
    "x0, v0 = 1.0, 0.0\n",
    "T = 10.0\n",
    "noise_scale = 0.005\n",
    "\n",
    "# Time grids\n",
    "t_plot = np.linspace(0, T, 400)\n",
    "t_obs = np.linspace(0, T, 21)\n",
    "\n",
    "# Ground truth and noisy observations\n",
    "x_plot = damped_oscillator_solution_np(t_plot, m, c_true, k_true, x0, v0)\n",
    "y_obs = damped_oscillator_solution_np(t_obs, m, c_true, k_true, x0, v0)\n",
    "y_obs += np.random.normal(scale=noise_scale, size=y_obs.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cpu\")\n",
    "t_obs_t, y_obs_t = [torch.tensor(arr.reshape(-1,1), dtype=torch.float32, device=device)\n",
    "                     for arr in (t_obs, y_obs)]\n",
    "\n",
    "# Collocation and initial condition points\n",
    "t_coll = torch.tensor(np.random.uniform(0, T, 120).reshape(-1,1), dtype=torch.float32, device=device)\n",
    "t_ic = torch.tensor([[0.0]], dtype=torch.float32, device=device)\n",
    "x0_t = torch.tensor([[x0]], dtype=torch.float32, device=device)\n",
    "v0_t = torch.tensor([[v0]], dtype=torch.float32, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8958c8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7. 🧩 PINN initialization --- \n",
    "model = PINN(hidden=64, layers=3).to(device)\n",
    "c_un = # 🧩 [TODO] define optimizable parameters \n",
    "k_un = # 🧩 [TODO] define optimizable parameters\n",
    "params = list(model.parameters()) + [c_un, k_un]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c2ad0",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755dd69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7. PINN initialization ---\n",
    "model = PINN(hidden=64, layers=3).to(device)\n",
    "c_un = nn.Parameter(torch.tensor(0.2, dtype=torch.float32, device=device))\n",
    "k_un = nn.Parameter(torch.tensor(2.0, dtype=torch.float32, device=device))\n",
    "params = list(model.parameters()) + [c_un, k_un]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98f9a5",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679fc2a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 9. Training loop (provided) ---\n",
    "t_coll.requires_grad_(True)  # Needed to compute derivatives for PDE residual\n",
    "t_ic.requires_grad_(True)    # Needed for IC derivative\n",
    "\n",
    "opt = torch.optim.AdamW(params, lr=1e-2)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_history = [] # for logging\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    opt.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Compute total loss and components\n",
    "    loss, lres, ldata, lic, c_val, k_val = loss_fn(\n",
    "        model, c_un, k_un, t_coll, t_obs_t, y_obs_t, t_ic, x0_t, v0_t, m\n",
    "    )\n",
    "\n",
    "    loss.backward()  # Backpropagation\n",
    "    opt.step()       # Update model and parameters\n",
    "\n",
    "    loss_history.append(loss.item()) # for loggin the loss curve\n",
    "\n",
    "    # Print progress every 200 epochs\n",
    "    if epoch == 1 or epoch % 200 == 0 or epoch == n_epochs:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"Total {loss.item():.4e} | \"\n",
    "            f\"Res {lres.item():.4e} | \"\n",
    "            f\"Data {ldata.item():.4e} | \"\n",
    "            f\"IC {lic.item():.4e} | \"\n",
    "            f\"c {c_val.item():.4f} | \"\n",
    "            f\"k {k_val.item():.4f}\"\n",
    "        )\n",
    "\n",
    "# plot losses\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b58d51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 10. Evaluation and visualization ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    t_plot_t = torch.tensor(t_plot.reshape(-1, 1), dtype=torch.float32, device=device)\n",
    "    x_pred = model(t_plot_t).cpu().numpy().flatten()\n",
    "    learned_c = c_un.item()\n",
    "    learned_k = k_un.item()\n",
    "\n",
    "print(f\"\\nTrue parameters:   c = {c_true:.4f}, k = {k_true:.4f}\")\n",
    "print(f\"Learned parameters: c = {learned_c:.4f}, k = {learned_k:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(t_plot, x_plot, label='True solution $x(t)$', linewidth=2)\n",
    "plt.plot(\n",
    "    t_plot, x_pred, linestyle='--', label='PINN prediction $\\\\hat{x}(t)$', linewidth=2\n",
    ")\n",
    "plt.scatter(\n",
    "    t_obs, y_obs, label='Sensor Data', color='red', edgecolor='k', s=40, zorder=5\n",
    ")\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Displacement $x(t)$')\n",
    "plt.title('PINN inference of $c$ and $k$')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b33303",
   "metadata": {},
   "source": [
    "### 3.6 Explore Transfer - Can Transfer Learning Be Used to Find the Solution Faster?\n",
    "\n",
    "**Idea:** Instead of training a PINN from scratch every time, we can **reuse a previously trained model** on a similar problem and **fine-tune** it for the new parameters.  \n",
    "\n",
    "- **Fine-tuning** means taking a model that already learned general features of the system (e.g., dynamics of the oscillator) and updating its weights slightly to adapt to a new scenario (e.g., different damping `c` or stiffness `k`).  \n",
    "- This often **converges faster** and requires **fewer collocation points** or epochs compared to training from scratch.  \n",
    "\n",
    "**Hands-on opportunity for students:**  \n",
    "- Load a pre-trained PINN for one set of parameters.  \n",
    "- Update `c` and `k` (or some layers of the network) and continue training on the new problem.  \n",
    "- Experiment with how many layers or parameters to freeze, and observe how it affects **speed and accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa71e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 6. Synthetic data for PINN training ---\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# System parameters\n",
    "m, c_true, k_true = 1.0, 0.5, 3.0 # <- ⚠️ Try changing this\n",
    "x0, v0 = 1.0, 0.0 # <- ⚠️ Try changing this\n",
    "T = 10.0\n",
    "noise_scale = 0.005\n",
    "\n",
    "# Time grids\n",
    "t_plot = np.linspace(0, T, 400)\n",
    "t_obs = np.linspace(0, T, 21)\n",
    "\n",
    "# Ground truth and noisy observations\n",
    "x_plot = damped_oscillator_solution_np(t_plot, m, c_true, k_true, x0, v0)\n",
    "y_obs = damped_oscillator_solution_np(t_obs, m, c_true, k_true, x0, v0)\n",
    "y_obs += np.random.normal(scale=noise_scale, size=y_obs.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cpu\")\n",
    "t_obs_t, y_obs_t = [torch.tensor(arr.reshape(-1,1), dtype=torch.float32, device=device)\n",
    "                     for arr in (t_obs, y_obs)]\n",
    "\n",
    "# Collocation and initial condition points\n",
    "t_coll = torch.tensor(np.random.uniform(0, T, 120).reshape(-1,1), dtype=torch.float32, device=device)\n",
    "t_ic = torch.tensor([[0.0]], dtype=torch.float32, device=device)\n",
    "x0_t = torch.tensor([[x0]], dtype=torch.float32, device=device)\n",
    "v0_t = torch.tensor([[v0]], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6d22f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7. PINN initialization ---\n",
    "# ⚠️ We skip this part to use previously trained model, but we save previous loss curve\n",
    "# ⚠️ Alternatively, try running it with a model initialized from scratch\n",
    "\n",
    "#model = PINN(hidden=64, layers=3).to(device)\n",
    "#c_un = nn.Parameter(torch.tensor(0.2, dtype=torch.float32, device=device))\n",
    "#k_un = nn.Parameter(torch.tensor(2.0, dtype=torch.float32, device=device))\n",
    "#params = list(model.parameters()) + [c_un, k_un]\n",
    "\n",
    "loss_pre_trained = loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ed7c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 9. Training loop (provided) ---\n",
    "t_coll.requires_grad_(True)  # Needed to compute derivatives for PDE residual\n",
    "t_ic.requires_grad_(True)    # Needed for IC derivative\n",
    "\n",
    "opt = torch.optim.AdamW(params, lr=1e-2)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_history = [] # for logging\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    opt.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Compute total loss and components\n",
    "    loss, lres, ldata, lic, c_val, k_val = loss_fn(\n",
    "        model, c_un, k_un, t_coll, t_obs_t, y_obs_t, t_ic, x0_t, v0_t, m\n",
    "    )\n",
    "\n",
    "    loss.backward()  # Backpropagation\n",
    "    opt.step()       # Update model and parameters\n",
    "\n",
    "    loss_history.append(loss.item()) # for loggin the loss curve\n",
    "\n",
    "    # Print progress every 200 epochs\n",
    "    if epoch == 1 or epoch % 200 == 0 or epoch == n_epochs:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"Total {loss.item():.4e} | \"\n",
    "            f\"Res {lres.item():.4e} | \"\n",
    "            f\"Data {ldata.item():.4e} | \"\n",
    "            f\"IC {lic.item():.4e} | \"\n",
    "            f\"c {c_val.item():.4f} | \"\n",
    "            f\"k {k_val.item():.4f}\"\n",
    "        )\n",
    "\n",
    "# plot losses\n",
    "plt.plot(loss_pre_trained, label='Pre-trained')\n",
    "plt.plot(loss_history, label='Fine-tuned')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6035a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 10. Evaluation and visualization ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    t_plot_t = torch.tensor(t_plot.reshape(-1, 1), dtype=torch.float32, device=device)\n",
    "    x_pred = model(t_plot_t).cpu().numpy().flatten()\n",
    "    learned_c = c_un.item()\n",
    "    learned_k = k_un.item()\n",
    "\n",
    "print(f\"\\nTrue parameters:   c = {c_true:.4f}, k = {k_true:.4f}\")\n",
    "print(f\"Learned parameters: c = {learned_c:.4f}, k = {learned_k:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(t_plot, x_plot, label='True solution $x(t)$', linewidth=2)\n",
    "plt.plot(\n",
    "    t_plot, x_pred, linestyle='--', label='PINN prediction $\\\\hat{x}(t)$', linewidth=2\n",
    ")\n",
    "plt.scatter(\n",
    "    t_obs, y_obs, label='Sensor Data', color='red', edgecolor='k', s=40, zorder=5\n",
    ")\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Displacement $x(t)$')\n",
    "plt.title('PINN inference of $c$ and $k$')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
