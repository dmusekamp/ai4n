{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92b76b3",
   "metadata": {},
   "source": [
    "# Beginner Tutorial: Machine Learning with PyTorch for Nuclear Engineering\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmusekamp/ai4n/blob/main/beginner_unsolved.ipynb)\n",
    "\n",
    "\n",
    "In this tutorial, we will learn the basic concepts of the python library PyTorch, by far the most popular library for machine learning in Python. First, we will introduce the basic concepts and operations of PyTorch, which we will use in the second part to tackle an example problem from Nuclear Engineering using a neural network.\n",
    "\n",
    "**Learning Goals**\n",
    "- Get familiar with basic PyTorch concepts\n",
    "- Understand the basics of regression with neural networks in PyTorch\n",
    "- Gain experience with data preprocessing and normalization\n",
    "- Learn how to evaluate and interpret model predictions in a scientific context\n",
    "\n",
    "This tutorial is in the form of a *jupyter* notebook hosted on *Google Colab*. Go through the tutorial step-by-step, read the instructions and try to understand the code. You have to execute each finished code block by pressing the play button. In the places with a *TODO* it is your task to fill out the blanks!\n",
    "\n",
    "Let's get started! Don't hesitate to ask us any questions!\n",
    "\n",
    "\n",
    "\n",
    "## Part I: Introduction to PyTorch Tensors and Operations\n",
    "\n",
    "\n",
    "Before we dive into building neural networks, let's get comfortable with the basics of PyTorch. In this section, you'll learn how to create and manipulate tensors, which are the fundamental data structure in PyTorch. You'll also see how PyTorch enables automatic differentiation, which is essential for training neural networks. This part of the tutorial was inspired and adapted from https://github.com/phlippe/uvadlc_notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd00d6",
   "metadata": {},
   "source": [
    "## What is a Tensor?\n",
    "A tensor is a multi-dimensional array, similar to a NumPy array, but with additional capabilities for GPU acceleration and automatic differentiation. Tensors are the building blocks for all PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b170e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b06a6",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "A tensor is a multi-dimensional array, similar to a NumPy array, but with additional capabilities for GPU acceleration and automatic differentiation. Tensors are the core blocks to represent data and values in PyTorch.\n",
    "\n",
    "You can create tensors from Python lists, NumPy arrays, or by using built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02843d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a Python list\n",
    "x_list = [1, 2, 3]\n",
    "x_tensor = torch.tensor(x_list) # a 1D tensor (a vector)\n",
    "print('Tensor from list:', x_tensor)\n",
    "\n",
    "# From a NumPy array\n",
    "x_np = np.array([[1, 2], [3, 4]]) # a 2D tensor (a matrix)\n",
    "x_tensor_from_np = torch.from_numpy(x_np)\n",
    "print('Tensor from NumPy array:')\n",
    "print(x_tensor_from_np)\n",
    "\n",
    "# Can also be transferred back to a NumPy array\n",
    "x_np_back = x_tensor_from_np.numpy()\n",
    "\n",
    "# We can also create a tensor with random numbers, zeros, or ones\n",
    "rand_tensor = torch.rand(2, 3)  # 2x3 matrix with random values\n",
    "zeros_tensor = torch.zeros(2, 2) # 2x2 matrix of zeros\n",
    "ones_tensor = torch.ones(1, 4) # 1x4 matrix of ones\n",
    "print('Random tensor:', rand_tensor)\n",
    "print('Zeros tensor:', zeros_tensor)\n",
    "print('Ones tensor:', ones_tensor)\n",
    "\n",
    "\n",
    "# We can get the shape of a tensor using the `.shape` attribute\n",
    "print('Shape of rand_tensor:', rand_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5feabe",
   "metadata": {},
   "source": [
    "## Tensor Operations\n",
    "PyTorch supports a wide range of mathematical operations on tensors, similar to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise addition\n",
    "print('a + b:', a + b)\n",
    "\n",
    "# Element-wise multiplication\n",
    "print('a * b:', a * b)\n",
    "\n",
    "# Matrix multiplication\n",
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 2)\n",
    "C = torch.matmul(A, B)\n",
    "print('Matrix multiplication result:', C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c25fd",
   "metadata": {},
   "source": [
    "We can also index a tensor to get the value at a certain position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 3)\n",
    "print(\"Second column\", x[:, 1])   # Second column\n",
    "print(\"First row\", x[0])      # First row\n",
    "print(\"Middle two rows\", x[1:3, :]) # Middle two rows\n",
    "\n",
    "#TODO: Get first two rows and the last column\n",
    "x2 = x[:2, -1]\n",
    "print(\"First two rows, last column\", x2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd66f6e",
   "metadata": {},
   "source": [
    "There a number of operations in Pytorch that reduce the shape of a tensor. We can use them by calling e.g.\n",
    "\n",
    "x.std(dim=1),\n",
    "\n",
    "on a tensor x which would replace the second dimension with the standard deviation of x. For a 2D tensor, this would give us the standard deviation of each row. If we don't give an argument, the function will calculate the standard deviation over all elements.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f9eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "Sum of all elements in x: tensor(15.)\n",
      "column mean tensor([1.5000, 2.5000, 3.5000])\n",
      "row max tensor([2., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2, 3).float()\n",
    "print(\"original\", x)\n",
    "# TODO: calculate the sum of all elements in x\n",
    "sum_x = \n",
    "print(\"Sum of all elements in x:\", sum_x)\n",
    "#TODO: calculate the mean of the column in x. Use dim=0 (we want to reduce the first dimension)\n",
    "x_mean = \n",
    "assert x_mean.shape == (3,), \"Mean along dim=0 should have shape (3,)\"\n",
    "print(\"column mean\", x_mean)\n",
    "#TODO: Calculate the maximum value of the row. Use dim=1\n",
    "x_max =\n",
    "assert x_max.shape == (2,), \"Max along dim=1 should have shape (2,)\"\n",
    "print(\"row max\", x_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9051ae",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "One of the main reasons for using PyTorch in Deep Learning projects is that we can automatically get **gradients/derivatives** of functions that we define. We will mainly use PyTorch for implementing neural networks, and they are just fancy functions. If we use weight matrices in our function that we want to learn, then those are called the **parameters** or simply the **weights**.\n",
    "\n",
    "If our neural network would output a single scalar value, we would talk about taking the **derivative**, but you will see that quite often we will have **multiple** output variables (\"values\"); in that case we talk about **gradients**. It's a more general term.\n",
    "\n",
    "Given an input $\\mathbf{x}$, we define our function by **manipulating** that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a **computational graph**. This graph shows how to arrive at our output from our input.\n",
    "PyTorch is a **define-by-run** framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us. Thus, we create a dynamic computation graph along the way.\n",
    "\n",
    "So, to recap: the only thing we have to do is to compute the **output**, and then we can ask PyTorch to automatically get the **gradients**.\n",
    "\n",
    "> **Note:  Why do we want gradients?** Consider that we have defined a function, a neural net, that is supposed to compute a certain output $y$ for an input vector $\\mathbf{x}$. We then define an **error measure** that tells us how wrong our network is; how bad it is in predicting output $y$ from input $\\mathbf{x}$. Based on this error measure, we can use the gradients to **update** the weights $\\mathbf{W}$ that were responsible for the output, so that the next time we present input $\\mathbf{x}$ to our network, the output will be closer to what we want.\n",
    "\n",
    "\n",
    "In order to get familiar with the concept of a computational graph, we will create one for the following function:\n",
    "\n",
    "$$y = \\sum_i \\left[(x_i + 2)^2 + 3\\right].$$\n",
    "\n",
    "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$. For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set requires_grad, so that PyTorch knows to store gradients with this tensor\n",
    "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
    "\n",
    "\n",
    "#TODO: implement the above function, use y.backward() at the end to compute the gradient dy/dx\n",
    "# (use .sum() in the end)\n",
    "\n",
    "y = \n",
    "\n",
    "print(\"Y\", y)\n",
    "\n",
    "print('Gradient dy/dx:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9e382",
   "metadata": {},
   "source": [
    "This is the one of the core ingredients of deep learning - to use autodiff to be able optimize the parameters of any (mathematically differentiable) model you can imagine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbd927",
   "metadata": {},
   "source": [
    "##  Using GPUs\n",
    "\n",
    "A crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks. When comparing GPUs to CPUs, we can list the following main differences (credit: [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/))\n",
    "\n",
    "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/comparison_CPU_GPU.png?raw=1\" width=\"700px\"></center>\n",
    "\n",
    "CPUs and GPUs have both different advantages and disadvantages, which is why many computers contain both components and use them for different tasks. In case you are not familiar with GPUs, you can read up more details in this [NVIDIA blog post](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) or [here](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html).\n",
    "\n",
    "GPUs can accelerate the training of your network  which is essential for large neural networks. PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn)). \n",
    "\n",
    "\n",
    "For this tutorial however, a GPU is not required. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b32d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_tensor.to('cuda')\n",
    "    print('Tensor on GPU:', x_gpu)\n",
    "    print(x_gpu.device)  # Should show that the tensor is on a CUDA device\n",
    "    # you can also send the results back to the cpu with .to('cpu')\n",
    "    x_cpu = x_gpu.to('cpu')\n",
    "    print('Tensor on CPU:', x_cpu,)\n",
    "    print(x_cpu.device)  # Should show that the tensor is on CPU\n",
    "else:\n",
    "    print('CUDA is not available on this machine.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b79ca4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now that you are familiar with the basics of PyTorch tensors and operations, you are ready to move on to Part II, where you will use these concepts to build and train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4381745",
   "metadata": {},
   "source": [
    "# Part II: Critical Heat Flux (CHF) Prediction\n",
    "\n",
    "The final goal of this tutorial is to write a simple neural network to solve a regression task from a nuclear engineering scenario. \n",
    "\n",
    "The data is part of the pyMAISE package (https://github.com/aims-umich/pyMAISE/) and was originally taken from the Nuclear Regulatory Commission CHF database. This dataset was generated from vertical water-cooled uniformly heated tubes, producing 24,579 samples from 59 different sources of measurements. The dataset was collected over experimental measurements spanning 60 years of CHF data collection methods such as visual identification, physical burnout, changes in the test section resistances, and the usage of thermocouples. The parameters collected consist of pressure ($P$), test section diameter ($D$), heated length ($L$), mass flux ($G$), inlet temperature ($T_{in}$), outlet equilibrium quality ($X$), and CHF. \n",
    "\n",
    "Negative quality $X$ could represent a subcooled fluid. The database was limited in terms of diameter (2 $<$ D $<$ 25 mm), $L/D$ ratio ($L/D$ $>$ 50 for X $>$ 0, $L/D$ $>$ 25 for X $<$ 0), pressure (100 $\\leq$ P $\\leq$ 21,000 kPa) and mass flux (0 $\\leq$ G $<$ 8,000 kg/m2/s). The measured data was also not equally distributed over the whole span, and no data beyond $D$ = 16 mm was found in the database.\n",
    "\n",
    " **Our goal is to predict CHF accurately given other geometrical parameters and boundary conditions ($D, L, P, G, T_{in}, X$).**\n",
    "\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "- `D (m)`: Diameter of the test section ($0.002 - 0.016~m$) \n",
    "- `L (m)`: Heated length ($0.07 - 15.0~m$)\n",
    "- `P (kPa)`: Pressure ($100 - 20,000~kPa$)\n",
    "- `G (kg m-2s-1)`: Mass flux ($17.7 - 7712.0~\\frac{kg}{m^2\\cdot s}$)\n",
    "- `Tin (C)`: Inlet temperature length ($9.0 - 353.62 ^\\circ C$)\n",
    "- `Xe (-)`: Outlet equilibrium quality ($-0.445 - 0.986$)\n",
    "\n",
    "**Output**\n",
    "\n",
    "- `CHF (kW m-2)`: Critical heat flux ($130.0 - 13345.0~\\frac{kW}{m^2}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105a7eb",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "We will use pandas for data handling, matplotlib for visualization, and PyTorch for building the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62527f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec9bf9",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the CHF Dataset\n",
    "Let's load the training and test datasets and take a quick look at the data. The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94641f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://raw.githubusercontent.com/aims-umich/pyMAISE/refs/heads/develop/pyMAISE/datasets/chf_train_synth.csv')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/aims-umich/pyMAISE/refs/heads/develop/pyMAISE/datasets/chf_test_synth.csv')\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Test shape:', test_df.shape)\n",
    "display(train_df.head())\n",
    "\n",
    "# Define feature columns and target\n",
    "feature_cols = ['D (m)', 'L (m)', 'P (kPa)', 'G (kg m-2s-1)', 'Tin (C)', 'Xe (-)']\n",
    "target_col = 'CHF (kW m-2)'\n",
    "\n",
    "X_train = train_df[feature_cols].values.astype(np.float32)\n",
    "X_test = test_df[feature_cols].values.astype(np.float32)\n",
    "y_train = train_df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
    "y_test = test_df[target_col].values.astype(np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d47618",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Data (Manual Standardization)\n",
    "As you can see, some of the inputs are really large, while other columns like the Diameter have a tiny scale. Such large and small values are not good for neural network traning. Ideally, we want all columns to be normalized so that they have a reasonable and equal magnitude.\n",
    "\n",
    "Therefore, we will separate features and targets, and standardize the features for better neural network performance using only PyTorch and NumPy. To convince yourself of the nessacity of normalization,  run the whole script again in the end without it to see what happens to the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8653c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Calculate the mean and std of X_train for each features (dim=0)\n",
    "X_train_mean = \n",
    "assert len(X_train_mean) == len(feature_cols), \"Mean shape is incorrect\"\n",
    "X_train_std = \n",
    "\n",
    "#TODO: Normalize the features by subtracting the mean and dividing by the standard deviation\n",
    "\n",
    "\n",
    "# Let's also standardize the targets, although it's not strictly necessary for a single-output regression\n",
    "# You can use tensor.mean() and use axis=0 to compute the mean along the correct axis\n",
    "y_train_mean = y_train.mean(axis=0)\n",
    "y_train_std = y_train.std(axis=0)\n",
    "y_train_scaled = (y_train - y_train_mean) / y_train_std\n",
    "y_test_scaled = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce3797",
   "metadata": {},
   "source": [
    "To efficiently train on mini-batches, we will use PyTorch's `TensorDataset` and `DataLoader` utilities. This allows us to easily shuffle and batch the data during training. While you can write your own Dataset for more involved usecases, we can use the default TensorDataset class for our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30392a",
   "metadata": {},
   "source": [
    "## Step 4: Build a Simple Neural Network for Regression\n",
    "\n",
    "In PyTorch, a neural network is defined as a class that inherits from `nn.Module`. The parameters (or weights) of the model are defined as layers inside the `__init__` method. The `forward` method defines how the input data flows through the network to produce an output. This is called the **forward pass**.\n",
    "\n",
    "For our first experiment, let's use the simplest possible neural network: a single linear layer. This is equivalent to linear regression, where the model learns a set of weights and a bias to map the input features to the output.\n",
    "\n",
    "- **Parameters:** The weights and bias of the linear layer are the parameters that will be learned during training. PyTorch automatically tracks these parameters for you when you use built-in layers like `nn.Linear`.\n",
    "- **Forward Pass:** The forward pass is the computation that takes the input tensor and produces the output tensor. For a linear layer, this is just a matrix multiplication plus a bias.\n",
    "\n",
    "Below is the code for a simple regression model using a single linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHFLinearRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A single linear layer. The first argument is the number of input features (6), \n",
    "        # and the second is the number of outputs (1) in the CHF example.\n",
    "        self.linear_layer = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_layer(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = CHFLinearRegressor()\n",
    "# Now, we can call the forward pass on a random input to get the prediction\n",
    "print(model(torch.rand(1, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b996c",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "- The model has 6 input features and 1 output.\n",
    "- The only parameters are the weights and bias of the linear layer.\n",
    "- The forward pass computes the output as a weighted sum of the inputs plus a bias.\n",
    "\n",
    "You can print the model to see its structure and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a180e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e3b2f",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model with Mini-batches\n",
    "First, we need to define a loss function. A loss function represents the cost function that we want to minimize. For a regression problem, we can use the mean squared error. It will take the current prediction and target values as inputs. If we would be able to reduce the mean squared error to zero, our model would be able to perfectly predict the targets in the training dataset. You can also find a variety of predefined loss functions in torch.nn. \n",
    "\n",
    "Here, we will use the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd39070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true):\n",
    "    # TODO: Try to Implement the mean squared error loss function.\n",
    "    mse = \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b8c80",
   "metadata": {},
   "source": [
    "The next ingredient is the optimizer. An optimizer in PyTorch is a class that changes the parameters (or weights) of our model using the calculated gradient. For example, move stochastic gradient descent (SGD) simply moves all weights in the direction of the gradient. We will use Adam, a more sophisticated version of SGD, which is the most commonly used optimizer. \"lr\" references the step size of our parameter update (learning rate). Higher learning rates may lead to faster training, but can also cause instabilities.\n",
    "\n",
    "\n",
    "After initializing the optimizer, we have to write the training loop. The training loop consists of a loop over the full dataset (an epoch). For each part of the full training dataset (batch), we update the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs=200):\n",
    "    # model.parameters() tells the optimizer which weights to optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # An epoch is one full pass through the training data. We will train for 200 epochs.\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  \n",
    "        epoch_loss = 0\n",
    "        # in each epoch, we iterate over mini-batches from the DataLoader\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Need to reset the gradients before running the backward pass.\n",
    "            optimizer.zero_grad()   \n",
    "\n",
    "            #TODO: Compute the gradient of the loss.\n",
    "            #TIP: Combine gradient calculation with the model forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * X_batch.size(0)        # for statistics\n",
    "\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training Loss Curve ')\n",
    "    plt.show()\n",
    "\n",
    "train(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92679939",
   "metadata": {},
   "source": [
    "## Step 6: Test\n",
    "\n",
    "Remember that we loaded two datasets in the beginning. \n",
    "\n",
    "The **training set** is used to fit the model parameters—this is the data the model \"sees\" and learns from during training. The **test set** is kept separate and is only used after training to evaluate how well the model generalizes to new, unseen data. This helps us estimate the real-world performance of our model. In the end, we only really interested in the error on the test set, since we only care about getting predictions for inputs for which we don't have the true, experimental output in our training data.\n",
    "\n",
    "In a proper machine learning workflow, you would also set aside a **validation set**. The validation set is used during model development to tune hyperparameters (like learning rate, number of layers, or neurons) and to prevent overfitting. You should only use the test set once, after all model choices are finalized, to get an unbiased estimate of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    y_test_true_list = []\n",
    "    y_test_pred_list = []\n",
    "\n",
    "    with torch.no_grad(): # We don't need gradients for evaluation -> saves memory  \n",
    "        # TODO: Iterate over the test_loader and collect predictions and true values\n",
    "        # TIP: Look at the training loop for reference, add to list using y_test_pred_list.append(...) and y_test_true_list.append(...)\n",
    "\n",
    "    y_test_pred = np.vstack(y_test_pred_list)\n",
    "    y_test_true = np.vstack(y_test_true_list)\n",
    "\n",
    "    # Calculate metrics in normalized space\n",
    "    mse = np.mean((y_test_true - y_test_pred) ** 2)\n",
    "    r2 = 1 - np.sum((y_test_true - y_test_pred) ** 2) / np.sum((y_test_true - np.mean(y_test_true)) ** 2)\n",
    "    print(f\"Test MSE (normalized): {mse:.4f}\")\n",
    "    print(f\"Test R2 (normalized): {r2:.4f}\")\n",
    "\n",
    "    plt.scatter(y_test_true, y_test_pred, alpha=0.6)\n",
    "    plt.xlabel('True CHF (normalized)')\n",
    "    plt.ylabel('Predicted CHF (normalized)')\n",
    "    plt.title('CHF Prediction: True vs. Predicted (Normalized)')\n",
    "    plt.plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'r--')\n",
    "    plt.show()\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724df97",
   "metadata": {},
   "source": [
    "## Step 7: A true neural network\n",
    "\n",
    "As you can see, the training error is quite high. The simple linear model is not able to predict the targets since the true target function is not linear. This is also called underfitting - the model is not expressive enough to follow the training distribution.\n",
    "\n",
    "A true neural network is not just a single linear layer - it should at least contain one *activation function*. A simple feed-forward-network consists of a sequence of linear layers and activation functions. The standard ReLu function for example simply implements a max: relu(x) = max(0, x). This non-linear layer allows the network to represent non-linear, complex functions. The Relu layer in Pytorch is *nn.ReLU()*.\n",
    "\n",
    "You should increase the output dimension of the first linear layer to e.g. 32. This is called the hidden dimension - we lift the data to a higher-dimensional space to make computations easier in the neural network. \n",
    "\n",
    "**TODO:** Implement the `CHFNeuralNetwork'. You should use three layers:\n",
    "1. A linear input layer that takes in the 6 input featuers and transform into the hidden_dim.\n",
    "2. The RelU activation function.\n",
    "3. A second linear layer to project from the hidden_dim to the single output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHFNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        #TODO: add the layer defintions. Take a look at the previous linear regression model for reference\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO: Call the layers in the correct order\n",
    "        return out\n",
    "\n",
    "model = CHFNeuralNetwork()\n",
    "train(model, train_loader)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64f165",
   "metadata": {},
   "source": [
    "## Step 8: Overfitting and Underfitting\n",
    "\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data. In our tutorial, the previous linear model is an example of underfitting: it cannot represent the complex, nonlinear relationship between the input features and the target variable (CHF). As a result, both the training and test errors remain high, and the model fails to make accurate predictions even on the data it was trained on.\n",
    "\n",
    "**Overfitting** happens when a model is too complex relative to the amount of available data. It learns not only the underlying patterns but also the noise in the training set, resulting in excellent performance on the training data but poor generalization to new, unseen data.\n",
    "\n",
    "In the following, we will simulate overfitting by using a larger model with only a subset of the dataset. See how the training loss compares to the last model above, and how the test loss changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff92c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CHFNeuralNetwork(512)\n",
    "\n",
    "\n",
    "#TODO: create a new TensorDataset and dataloader\n",
    "# Use only the first 50 samples from X_train_tensor and y_train_tensor\n",
    "\n",
    "\n",
    "small_dataset = \n",
    "small_train_loader = \n",
    "\n",
    "assert len(small_dataset) == 50, \"Small dataset should contain 50 samples\"\n",
    "\n",
    "train(model, small_train_loader, n_epochs=2000)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a721a",
   "metadata": {},
   "source": [
    "## Step 9: Experiment\n",
    "\n",
    "Nice! We solved a real-world machine learning task with a simple neural network.\n",
    "\n",
    "This is just the start - for a problem of this size you could also use different machine learning models (e.g. Gaussian Processess). Another thing to consider is model uncertainty - can we get a model that can estimate its own error on test inputs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
